{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# Connexion à Hugging Face\n",
        "login(\"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\")\n",
        "\n",
        "# Téléchargement des ressources NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "csv_path = '/content/drive/MyDrive/Annotation_final.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Initialisation du stemmer anglais\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Fonction de nettoyage et stemming\n",
        "def nettoyer_texte_stemmer(texte):\n",
        "    texte = str(texte).lower()\n",
        "    texte = re.sub(r\"http\\S+|www\\S+\", \"\", texte)\n",
        "    texte = re.sub(r\"@\\w+|#\\w+\", \"\", texte)\n",
        "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
        "    mots = texte.split()\n",
        "    mots_stemmes = [stemmer.stem(mot) for mot in mots]\n",
        "    return \" \".join(mots_stemmes)\n",
        "\n",
        "# Application du prétraitement\n",
        "df['texte_nettoye'] = df['text'].apply(nettoyer_texte_stemmer)\n",
        "\n",
        "# Features et labels\n",
        "X = df['texte_nettoye']\n",
        "y = df['emotions']\n",
        "\n",
        "# Vectorisation TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Initialisation des modèles\n",
        "clf_nb = MultinomialNB()\n",
        "clf_svm = LinearSVC()\n",
        "\n",
        "# Validation croisée\n",
        "cv = 5\n",
        "\n",
        "# Initialisation du modèle Logistic Regression\n",
        "clf_logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Validation croisée pour Logistic Regression\n",
        "scores_logreg_acc = cross_val_score(clf_logreg, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_logreg_prec = cross_val_score(clf_logreg, X_vec, y, cv=cv, scoring='precision_weighted')\n",
        "scores_logreg_rec = cross_val_score(clf_logreg, X_vec, y, cv=cv, scoring='recall_weighted')\n",
        "scores_logreg_f1 = cross_val_score(clf_logreg, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "# Naive Bayes\n",
        "scores_nb_acc = cross_val_score(clf_nb, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_nb_prec = cross_val_score(clf_nb, X_vec, y, cv=cv, scoring='precision_weighted')\n",
        "scores_nb_rec = cross_val_score(clf_nb, X_vec, y, cv=cv, scoring='recall_weighted')\n",
        "scores_nb_f1 = cross_val_score(clf_nb, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "# SVM Linéaire\n",
        "scores_svm_acc = cross_val_score(clf_svm, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_svm_prec = cross_val_score(clf_svm, X_vec, y, cv=cv, scoring='precision_weighted')\n",
        "scores_svm_rec = cross_val_score(clf_svm, X_vec, y, cv=cv, scoring='recall_weighted')\n",
        "scores_svm_f1 = cross_val_score(clf_svm, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"\\n===== Moyennes et écarts types (cross-validation, Naive Bayes) =====\")\n",
        "print(f\"Exactitude : {scores_nb_acc.mean():.3f} ± {scores_nb_acc.std():.3f}\")\n",
        "print(f\"Précision  : {scores_nb_prec.mean():.3f} ± {scores_nb_prec.std():.3f}\")\n",
        "print(f\"Rappel     : {scores_nb_rec.mean():.3f} ± {scores_nb_rec.std():.3f}\")\n",
        "print(f\"F1-score   : {scores_nb_f1.mean():.3f} ± {scores_nb_f1.std():.3f}\")\n",
        "\n",
        "print(\"\\n===== Moyennes et écarts types (cross-validation, SVM Linéaire) =====\")\n",
        "print(f\"Exactitude : {scores_svm_acc.mean():.3f} ± {scores_svm_acc.std():.3f}\")\n",
        "print(f\"Précision  : {scores_svm_prec.mean():.3f} ± {scores_svm_prec.std():.3f}\")\n",
        "print(f\"Rappel     : {scores_svm_rec.mean():.3f} ± {scores_svm_rec.std():.3f}\")\n",
        "print(f\"F1-score   : {scores_svm_f1.mean():.3f} ± {scores_svm_f1.std():.3f}\")\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"\\n===== Moyennes et écarts types (cross-validation, Logistic Regression) =====\")\n",
        "print(f\"Exactitude : {scores_logreg_acc.mean():.3f} ± {scores_logreg_acc.std():.3f}\")\n",
        "print(f\"Précision  : {scores_logreg_prec.mean():.3f} ± {scores_logreg_prec.std():.3f}\")\n",
        "print(f\"Rappel     : {scores_logreg_rec.mean():.3f} ± {scores_logreg_rec.std():.3f}\")\n",
        "print(f\"F1-score   : {scores_logreg_f1.mean():.3f} ± {scores_logreg_f1.std():.3f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45ssUg46EGuR",
        "outputId": "c7d585bf-4eae-4b03-ab54-d0d72680dbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Moyennes et écarts types (cross-validation, Naive Bayes) =====\n",
            "Exactitude : 0.426 ± 0.004\n",
            "Précision  : 0.251 ± 0.141\n",
            "Rappel     : 0.426 ± 0.004\n",
            "F1-score   : 0.257 ± 0.008\n",
            "\n",
            "===== Moyennes et écarts types (cross-validation, SVM Linéaire) =====\n",
            "Exactitude : 0.482 ± 0.026\n",
            "Précision  : 0.418 ± 0.035\n",
            "Rappel     : 0.482 ± 0.026\n",
            "F1-score   : 0.414 ± 0.029\n",
            "\n",
            "===== Moyennes et écarts types (cross-validation, Logistic Regression) =====\n",
            "Exactitude : 0.453 ± 0.015\n",
            "Précision  : 0.382 ± 0.070\n",
            "Rappel     : 0.453 ± 0.015\n",
            "F1-score   : 0.319 ± 0.022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "# Connexion à Hugging Face\n",
        "login(\"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\")\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "csv_path = '/content/drive/MyDrive/Annotation_final.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Nettoyage simple sans stemming\n",
        "def nettoyer_texte(texte):\n",
        "    texte = str(texte).lower()\n",
        "    texte = re.sub(r\"http\\S+|www\\S+\", \"\", texte)\n",
        "    texte = re.sub(r\"@\\w+|#\\w+\", \"\", texte)\n",
        "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
        "    return texte\n",
        "\n",
        "df['texte_nettoye'] = df['text'].apply(nettoyer_texte)\n",
        "X = df['texte_nettoye']\n",
        "y = df['emotions']\n",
        "\n",
        "# Vectorisation TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Initialisation des modèles\n",
        "clf_nb = MultinomialNB()\n",
        "clf_svm = LinearSVC()\n",
        "clf_logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Définition des pipelines pour SMOTE\n",
        "pipeline_smote_nb = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', clf_nb)\n",
        "])\n",
        "pipeline_smote_svm = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', clf_svm)\n",
        "])\n",
        "pipeline_smote_logreg = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', clf_logreg)\n",
        "])\n",
        "\n",
        "# Définition des pipelines pour RandomOverSampler\n",
        "pipeline_random_nb = Pipeline([\n",
        "    ('random', RandomOverSampler(random_state=42)),\n",
        "    ('clf', clf_nb)\n",
        "])\n",
        "pipeline_random_svm = Pipeline([\n",
        "    ('random', RandomOverSampler(random_state=42)),\n",
        "    ('clf', clf_svm)\n",
        "])\n",
        "pipeline_random_logreg = Pipeline([\n",
        "    ('random', RandomOverSampler(random_state=42)),\n",
        "    ('clf', clf_logreg)\n",
        "])\n",
        "\n",
        "# Cross-validation stratifiée\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Évaluation SMOTE\n",
        "scores_smote_nb_acc = cross_val_score(pipeline_smote_nb, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_smote_nb_f1 = cross_val_score(pipeline_smote_nb, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "scores_smote_svm_acc = cross_val_score(pipeline_smote_svm, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_smote_svm_f1 = cross_val_score(pipeline_smote_svm, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "scores_smote_logreg_acc = cross_val_score(pipeline_smote_logreg, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_smote_logreg_f1 = cross_val_score(pipeline_smote_logreg, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "# Évaluation RandomOverSampler\n",
        "scores_random_nb_acc = cross_val_score(pipeline_random_nb, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_random_nb_f1 = cross_val_score(pipeline_random_nb, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "scores_random_svm_acc = cross_val_score(pipeline_random_svm, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_random_svm_f1 = cross_val_score(pipeline_random_svm, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "scores_random_logreg_acc = cross_val_score(pipeline_random_logreg, X_vec, y, cv=cv, scoring='accuracy')\n",
        "scores_random_logreg_f1 = cross_val_score(pipeline_random_logreg, X_vec, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "# Préparation des résultats pour affichage\n",
        "results = {\n",
        "    'SMOTE': {\n",
        "        'Naive Bayes': {'accuracy': scores_smote_nb_acc, 'f1': scores_smote_nb_f1},\n",
        "        'SVM': {'accuracy': scores_smote_svm_acc, 'f1': scores_smote_svm_f1},\n",
        "        'Logistic Regression': {'accuracy': scores_smote_logreg_acc, 'f1': scores_smote_logreg_f1}\n",
        "    },\n",
        "    'RandomOverSampler': {\n",
        "        'Naive Bayes': {'accuracy': scores_random_nb_acc, 'f1': scores_random_nb_f1},\n",
        "        'SVM': {'accuracy': scores_random_svm_acc, 'f1': scores_random_svm_f1},\n",
        "        'Logistic Regression': {'accuracy': scores_random_logreg_acc, 'f1': scores_random_logreg_f1}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Affichage des résultats\n",
        "for method, models in results.items():\n",
        "    print(f\"\\n===== Résultats avec {method} =====\")\n",
        "    for model_name, scores in models.items():\n",
        "        print(f\"\\nModèle : {model_name}\")\n",
        "        print(f\"Exactitude : {scores['accuracy'].mean():.3f} ± {scores['accuracy'].std():.3f}\")\n",
        "        print(f\"F1-score   : {scores['f1'].mean():.3f} ± {scores['f1'].std():.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paqo8OL8bby_",
        "outputId": "d7b3351f-fb4a-46cc-cdac-9e39c47e5cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Résultats avec SMOTE =====\n",
            "\n",
            "Modèle : Naive Bayes\n",
            "Exactitude : 0.277 ± 0.023\n",
            "F1-score   : 0.282 ± 0.036\n",
            "\n",
            "Modèle : SVM\n",
            "Exactitude : 0.448 ± 0.026\n",
            "F1-score   : 0.409 ± 0.023\n",
            "\n",
            "Modèle : Logistic Regression\n",
            "Exactitude : 0.464 ± 0.005\n",
            "F1-score   : 0.420 ± 0.010\n",
            "\n",
            "===== Résultats avec RandomOverSampler =====\n",
            "\n",
            "Modèle : Naive Bayes\n",
            "Exactitude : 0.275 ± 0.019\n",
            "F1-score   : 0.288 ± 0.023\n",
            "\n",
            "Modèle : SVM\n",
            "Exactitude : 0.453 ± 0.026\n",
            "F1-score   : 0.413 ± 0.024\n",
            "\n",
            "Modèle : Logistic Regression\n",
            "Exactitude : 0.459 ± 0.019\n",
            "F1-score   : 0.414 ± 0.016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "csv_path = '/content/drive/MyDrive/Annotation_final.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Nettoyage simple sans stemming\n",
        "def nettoyer_texte(texte):\n",
        "    texte = str(texte).lower()\n",
        "    texte = re.sub(r\"http\\S+|www\\S+\", \"\", texte)\n",
        "    texte = re.sub(r\"@\\w+|#\\w+\", \"\", texte)\n",
        "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
        "    return texte\n",
        "\n",
        "df['texte_nettoye'] = df['text'].apply(nettoyer_texte)\n",
        "X = df['texte_nettoye']\n",
        "y = df['emotions']\n",
        "\n",
        "# Pipeline et grille de paramètres pour Naive Bayes\n",
        "pipeline_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "param_grid_nb = {\n",
        "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
        "    'tfidf__max_df': [0.9, 1.0],\n",
        "    'tfidf__min_df': [1, 3],\n",
        "    'clf__alpha': [0.1, 1.0, 5.0],\n",
        "    'clf__fit_prior': [True, False]    # <--- Ajout de fit_prior ici\n",
        "}\n",
        "\n",
        "# Pipeline et grille de paramètres pour SVM\n",
        "pipeline_svm = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "param_grid_svm = {\n",
        "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
        "    'clf__C': [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Pipeline et grille de paramètres pour Logistic Regression\n",
        "pipeline_logreg = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "param_grid_logreg = {\n",
        "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
        "    'clf__C': [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Création des objets GridSearchCV\n",
        "grid_search_nb = GridSearchCV(pipeline_nb, param_grid_nb, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search_svm = GridSearchCV(pipeline_svm, param_grid_svm, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search_logreg = GridSearchCV(pipeline_logreg, param_grid_logreg, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Entraînement des modèles\n",
        "print(\"Entraînement Naive Bayes...\")\n",
        "grid_search_nb.fit(X, y)\n",
        "print(\"Entraînement SVM...\")\n",
        "grid_search_svm.fit(X, y)\n",
        "print(\"Entraînement Logistic Regression...\")\n",
        "grid_search_logreg.fit(X, y)\n",
        "\n",
        "# Résultats\n",
        "print(\"Naive Bayes meilleurs paramètres :\", grid_search_nb.best_params_)\n",
        "print(\"Naive Bayes meilleur score F1 :\", grid_search_nb.best_score_)\n",
        "\n",
        "print(\"SVM meilleurs paramètres :\", grid_search_svm.best_params_)\n",
        "print(\"SVM meilleur score F1 :\", grid_search_svm.best_score_)\n",
        "\n",
        "print(\"LogReg meilleurs paramètres :\", grid_search_logreg.best_params_)\n",
        "print(\"LogReg meilleur score F1 :\", grid_search_logreg.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ibpJQhzt65l",
        "outputId": "8d5787c3-d034-412b-93f5-6073c7f5bb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entraînement Naive Bayes...\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "Entraînement SVM...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Entraînement Logistic Regression...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Naive Bayes meilleurs paramètres : {'clf__alpha': 0.1, 'clf__fit_prior': True, 'tfidf__max_df': 0.9, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1)}\n",
            "Naive Bayes meilleur score F1 : 0.357062688585719\n",
            "SVM meilleurs paramètres : {'clf__C': 1, 'tfidf__ngram_range': (1, 1)}\n",
            "SVM meilleur score F1 : 0.4149556215762634\n",
            "LogReg meilleurs paramètres : {'clf__C': 1, 'tfidf__ngram_range': (1, 1)}\n",
            "LogReg meilleur score F1 : 0.41536647207112487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Connexion à Hugging Face\n",
        "login(\"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\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "csv_path = '/content/drive/MyDrive/Annotation_final.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Nettoyage et stemming\n",
        "stemmer = PorterStemmer()\n",
        "def nettoyer_texte_stemmer(texte):\n",
        "    texte = str(texte).lower()\n",
        "    texte = re.sub(r\"http\\S+|www\\S+\", \"\", texte)\n",
        "    texte = re.sub(r\"@\\w+|#\\w+\", \"\", texte)\n",
        "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
        "    mots = texte.split()\n",
        "    mots_stemmes = [stemmer.stem(mot) for mot in mots]\n",
        "    return \" \".join(mots_stemmes)\n",
        "df['texte_nettoye'] = df['text'].apply(nettoyer_texte_stemmer)\n",
        "\n",
        "X = df['texte_nettoye']\n",
        "y = df['emotions']\n",
        "\n",
        "# Vectorisation TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Modèles\n",
        "clf_nb = MultinomialNB()\n",
        "clf_svm = LinearSVC()\n",
        "\n",
        "# Sur-échantillonneurs\n",
        "smote = SMOTE(random_state=42)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Métriques\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
        "    'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
        "    'f1': make_scorer(f1_score, average='weighted', zero_division=0)\n",
        "}\n",
        "cv = 5\n",
        "\n",
        "# Pipelines\n",
        "pipeline_nb_smote = ImbPipeline([('smote', smote), ('clf', clf_nb)])\n",
        "pipeline_nb_ros = ImbPipeline([('ros', ros), ('clf', clf_nb)])\n",
        "pipeline_svm_smote = ImbPipeline([('smote', smote), ('clf', clf_svm)])\n",
        "pipeline_svm_ros = ImbPipeline([('ros', ros), ('clf', clf_svm)])\n",
        "\n",
        "# Validation croisée\n",
        "scores_nb_smote = cross_validate(pipeline_nb_smote, X_vec, y, cv=cv, scoring=scoring)\n",
        "scores_nb_ros = cross_validate(pipeline_nb_ros, X_vec, y, cv=cv, scoring=scoring)\n",
        "scores_svm_smote = cross_validate(pipeline_svm_smote, X_vec, y, cv=cv, scoring=scoring)\n",
        "scores_svm_ros = cross_validate(pipeline_svm_ros, X_vec, y, cv=cv, scoring=scoring)\n",
        "\n",
        "def afficher_scores(scores, nom):\n",
        "    print(f\"\\n===== Moyennes et écarts types ({nom}) =====\")\n",
        "    for metric in ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']:\n",
        "        mean = np.mean(scores[metric])\n",
        "        std = np.std(scores[metric])\n",
        "        print(f\"{metric[5:].capitalize()} : {mean:.3f} ± {std:.3f}\")\n",
        "\n",
        "# Résultats\n",
        "afficher_scores(scores_nb_smote, \"Naive Bayes + SMOTE\")\n",
        "afficher_scores(scores_nb_ros, \"Naive Bayes + RandomOverSampler\")\n",
        "afficher_scores(scores_svm_smote, \"SVM + SMOTE\")\n",
        "afficher_scores(scores_svm_ros, \"SVM + RandomOverSampler\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV7BlyisO6_k",
        "outputId": "f3ff2b8a-ff2d-48f4-e31b-2bc1b885e12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Moyennes et écarts types (Naive Bayes + SMOTE) =====\n",
            "Accuracy : 0.290 ± 0.025\n",
            "Precision : 0.437 ± 0.062\n",
            "Recall : 0.290 ± 0.025\n",
            "F1 : 0.297 ± 0.027\n",
            "\n",
            "===== Moyennes et écarts types (Naive Bayes + RandomOverSampler) =====\n",
            "Accuracy : 0.307 ± 0.025\n",
            "Precision : 0.442 ± 0.030\n",
            "Recall : 0.307 ± 0.025\n",
            "F1 : 0.322 ± 0.030\n",
            "\n",
            "===== Moyennes et écarts types (SVM + SMOTE) =====\n",
            "Accuracy : 0.464 ± 0.022\n",
            "Precision : 0.428 ± 0.027\n",
            "Recall : 0.464 ± 0.022\n",
            "F1 : 0.430 ± 0.026\n",
            "\n",
            "===== Moyennes et écarts types (SVM + RandomOverSampler) =====\n",
            "Accuracy : 0.459 ± 0.019\n",
            "Precision : 0.425 ± 0.027\n",
            "Recall : 0.459 ± 0.019\n",
            "F1 : 0.425 ± 0.027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Mapping des émotions numériques vers leurs noms\n",
        "mapping_emotions = {\n",
        "    1: 'fear',\n",
        "    2: 'anger',\n",
        "    3: 'joy',\n",
        "    4: 'surprise',\n",
        "    5: 'sadness',\n",
        "    6: 'disgust',\n",
        "    7: 'neutral'\n",
        "}\n",
        "\n",
        "# Mapping basé sur les définitions du dictionnaire\n",
        "emoroberta_to_our_mapping = {\n",
        "    # Colère (Anger) - sentiment violent de mécontentement\n",
        "    'anger': 2,  # colère directe\n",
        "    'annoyance': 2,  # irritation, agacement\n",
        "    'disapproval': 2,  # désapprobation (forme de mécontentement)\n",
        "\n",
        "    # Dégoût (Disgust) - aversion profonde\n",
        "    'disgust': 6,  # dégoût direct\n",
        "    'remorse': 6,  # remords (forme de dégoût envers soi-même)\n",
        "\n",
        "    # Peur (Fear) - sentiment d'inquiétude face à un danger\n",
        "    'fear': 1,  # peur directe\n",
        "    'nervousness': 1,  # nervosité (forme d'anxiété)\n",
        "    'embarrassment': 1,  # embarras (peur du jugement social)\n",
        "\n",
        "    # Joie (Joy) - sentiment de plaisir, de bonheur\n",
        "    'joy': 3,  # joie directe\n",
        "    'amusement': 3,  # amusement (plaisir léger)\n",
        "    'excitement': 3,  # excitation (joie anticipatoire)\n",
        "    'gratitude': 3,  # gratitude (joie reconnaissante)\n",
        "    'love': 3,  # amour (joie profonde liée à l'attachement)\n",
        "    'optimism': 3,  # optimisme (joie anticipatoire)\n",
        "    'pride': 3,  # fierté (joie liée à l'accomplissement)\n",
        "    'relief': 3,  # soulagement (joie après tension)\n",
        "    'desire': 3,  # désir (joie anticipatoire)\n",
        "    'admiration': 3,  # admiration (joie face à quelque chose d'excellent)\n",
        "    'caring': 3,  # attention/soin (forme d'amour, donc joie)\n",
        "\n",
        "    # Neutre (Neutral) - absence d'émotion marquée\n",
        "    'neutral': 7,  # neutre direct\n",
        "    'approval': 7,  # approbation (peut être considérée comme neutre ou légèrement positive)\n",
        "    'realization': 7,  # réalisation (prise de conscience, souvent neutre)\n",
        "\n",
        "    # Tristesse (Sadness) - état de chagrin, de mélancolie\n",
        "    'sadness': 5,  # tristesse directe\n",
        "    'disappointment': 5,  # déception (tristesse face à des attentes non comblées)\n",
        "    'grief': 5,  # chagrin (tristesse intense liée à une perte)\n",
        "\n",
        "    # Surprise (Surprise) - étonnement soudain\n",
        "    'surprise': 4,  # surprise directe\n",
        "    'confusion': 4,  # confusion (surprise désorganisante)\n",
        "    'curiosity': 4,  # curiosité (surprise qui pousse à l'exploration)\n",
        "}\n",
        "\n",
        "# Chargement du modèle et du tokenizer\n",
        "print(\"Chargement du modèle et du tokenizer EmoRoBERTa...\")\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
        "model = TFRobertaForSequenceClassification.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
        "emotion_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "csv_path = '/content/drive/MyDrive/Annotation_final.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Initialisation du stemmer anglais\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Fonction de nettoyage et stemming\n",
        "def nettoyer_texte_stemmer(texte):\n",
        "    texte = str(texte).lower()\n",
        "    texte = re.sub(r\"http\\S+|www\\S+\", \"\", texte)\n",
        "    texte = re.sub(r\"@\\w+|#\\w+\", \"\", texte)\n",
        "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
        "    mots = texte.split()\n",
        "    mots_stemmes = [stemmer.stem(mot) for mot in mots]\n",
        "    return \" \".join(mots_stemmes)\n",
        "\n",
        "# Application du prétraitement\n",
        "print(\"Application du prétraitement...\")\n",
        "df['texte_nettoye'] = df['text'].apply(nettoyer_texte_stemmer)\n",
        "\n",
        "# Limitation à 100 entrées pour les tests\n",
        "#df = df.head(300)\n",
        "\n",
        "# Prédiction des émotions avec EmoRoBERTa\n",
        "print(\"Prédiction des émotions...\")\n",
        "resultats = []\n",
        "\n",
        "for texte in df['texte_nettoye']:\n",
        "    try:\n",
        "        prediction = emotion_pipeline(texte)[0]  # Accéder au premier élément de la liste\n",
        "        emotion_label = prediction['label']\n",
        "        emotion_num = emoroberta_to_our_mapping.get(emotion_label, 0)\n",
        "        resultats.append({\n",
        "            'texte': texte,\n",
        "            'emotion_originale': emotion_label,\n",
        "            'emotion_num': emotion_num,\n",
        "            'emotion': mapping_emotions.get(emotion_num, 'inconnu'),\n",
        "            'score': prediction['score']\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la prédiction pour le texte: {texte}\")\n",
        "        print(f\"Erreur: {e}\")\n",
        "        resultats.append({\n",
        "            'texte': texte,\n",
        "            'emotion_originale': 'erreur',\n",
        "            'emotion_num': 0,\n",
        "            'emotion': 'erreur',\n",
        "            'score': 0.0\n",
        "        })\n",
        "\n",
        "# Création d'un DataFrame avec les résultats\n",
        "resultats_df = pd.DataFrame(resultats)\n",
        "\n",
        "# Fusion avec le DataFrame original\n",
        "df_final = pd.concat([df, resultats_df[['emotion_originale', 'emotion_num', 'emotion', 'score']]], axis=1)\n",
        "\n",
        "# Évaluation des performances avec la colonne 'emotions' comme référence\n",
        "print(\"Évaluation des performances avec la colonne 'emotions'...\")\n",
        "y_true = df['emotions']\n",
        "y_pred = resultats_df['emotion_num']\n",
        "\n",
        "print(\"Rapport de classification:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[mapping_emotions[i] for i in sorted(mapping_emotions.keys())]))\n",
        "\n",
        "print(f\"Précision: {precision_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"Rappel: {recall_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"F1-score: {f1_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"Exactitude: {accuracy_score(y_true, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxK5di0F8SO8",
        "outputId": "358c19f2-c7e9-4318-9a2e-db4daa9999fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement du modèle et du tokenizer EmoRoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at arpanghoshal/EmoRoBERTa.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "Device set to use 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Application du prétraitement...\n",
            "Prédiction des émotions...\n",
            "Évaluation des performances avec la colonne 'emotions'...\n",
            "Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.17      0.04      0.06        52\n",
            "       anger       0.63      0.18      0.29       184\n",
            "         joy       0.49      0.38      0.42       218\n",
            "    surprise       0.15      0.11      0.13        91\n",
            "     sadness       0.50      0.14      0.22        65\n",
            "     disgust       0.00      0.00      0.00        46\n",
            "     neutral       0.48      0.82      0.61       484\n",
            "\n",
            "    accuracy                           0.47      1140\n",
            "   macro avg       0.35      0.24      0.25      1140\n",
            "weighted avg       0.45      0.47      0.41      1140\n",
            "\n",
            "Précision: 0.4491227785437235\n",
            "Rappel: 0.4666666666666667\n",
            "F1-score: 0.41101380881875754\n",
            "Exactitude: 0.4666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Mapping des émotions numériques vers leurs noms\n",
        "mapping_emotions = {\n",
        "    1: 'fear',\n",
        "    2: 'anger',\n",
        "    3: 'joy',\n",
        "    4: 'surprise',\n",
        "    5: 'sadness',\n",
        "    6: 'disgust',\n",
        "    7: 'neutral'\n",
        "}\n",
        "\n",
        "# Mapping inverse pour la comparaison\n",
        "mapping_inverse = {\n",
        "    'fear': 1,\n",
        "    'anger': 2,\n",
        "    'joy': 3,\n",
        "    'surprise': 4,\n",
        "    'sadness': 5,\n",
        "    'disgust': 6,\n",
        "    'neutral': 7\n",
        "}\n",
        "\n",
        "# Chargement du modèle et du tokenizer\n",
        "print(\"Chargement du modèle et du tokenizer DistilRoBERTa...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
        "emotion_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "csv_path = '/content/drive/MyDrive/Annotation_final.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Initialisation du stemmer anglais\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Fonction de nettoyage et stemming\n",
        "def nettoyer_texte_stemmer(texte):\n",
        "    texte = str(texte).lower()\n",
        "    texte = re.sub(r\"http\\S+|www\\S+\", \"\", texte)\n",
        "    texte = re.sub(r\"@\\w+|#\\w+\", \"\", texte)\n",
        "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
        "    mots = texte.split()\n",
        "    mots_stemmes = [stemmer.stem(mot) for mot in mots]\n",
        "    return \" \".join(mots_stemmes)\n",
        "\n",
        "# Application du prétraitement\n",
        "print(\"Application du prétraitement...\")\n",
        "df['texte_nettoye'] = df['text'].apply(nettoyer_texte_stemmer)\n",
        "\n",
        "# Limitation à 300 entrées pour les tests\n",
        "#df = df.head(500)\n",
        "\n",
        "# Prédiction des émotions avec DistilRoBERTa\n",
        "print(\"Prédiction des émotions...\")\n",
        "resultats = []\n",
        "\n",
        "for texte in df['texte_nettoye']:\n",
        "    try:\n",
        "        prediction = emotion_pipeline(texte)[0]\n",
        "        emotion_label = prediction['label']\n",
        "        emotion_num = mapping_inverse.get(emotion_label, 0)\n",
        "        resultats.append({\n",
        "            'texte': texte,\n",
        "            'emotion_originale': emotion_label,\n",
        "            'emotion_num': emotion_num,\n",
        "            'emotion': mapping_emotions.get(emotion_num, 'inconnu'),\n",
        "            'score': prediction['score']\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la prédiction pour le texte: {texte}\")\n",
        "        print(f\"Erreur: {e}\")\n",
        "        resultats.append({\n",
        "            'texte': texte,\n",
        "            'emotion_originale': 'erreur',\n",
        "            'emotion_num': 0,\n",
        "            'emotion': 'erreur',\n",
        "            'score': 0.0\n",
        "        })\n",
        "\n",
        "# Création d'un DataFrame avec les résultats\n",
        "resultats_df = pd.DataFrame(resultats)\n",
        "\n",
        "# Fusion avec le DataFrame original\n",
        "df_final = pd.concat([df, resultats_df[['emotion_originale', 'emotion_num', 'emotion', 'score']]], axis=1)\n",
        "\n",
        "# Évaluation des performances avec la colonne 'emotions' comme référence\n",
        "print(\"Évaluation des performances avec la colonne 'emotions'...\")\n",
        "y_true = df['emotions']\n",
        "y_pred = resultats_df['emotion_num']\n",
        "\n",
        "print(\"Rapport de classification:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[mapping_emotions[i] for i in sorted(mapping_emotions.keys())]))\n",
        "\n",
        "print(f\"Précision: {precision_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"Rappel: {recall_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"F1-score: {f1_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"Exactitude: {accuracy_score(y_true, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJbo6m7oAH2f",
        "outputId": "521c3097-28d6-4083-d631-fd2b02f99e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement du modèle et du tokenizer DistilRoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Application du prétraitement...\n",
            "Prédiction des émotions...\n",
            "Évaluation des performances avec la colonne 'emotions'...\n",
            "Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.26      0.38      0.31        52\n",
            "       anger       0.59      0.43      0.50       184\n",
            "         joy       0.60      0.37      0.46       218\n",
            "    surprise       0.21      0.21      0.21        91\n",
            "     sadness       0.19      0.37      0.25        65\n",
            "     disgust       0.18      0.04      0.07        46\n",
            "     neutral       0.57      0.66      0.61       484\n",
            "\n",
            "    accuracy                           0.48      1140\n",
            "   macro avg       0.37      0.35      0.34      1140\n",
            "weighted avg       0.50      0.48      0.48      1140\n",
            "\n",
            "Précision: 0.49759629173566106\n",
            "Rappel: 0.4780701754385965\n",
            "F1-score: 0.47518895264540567\n",
            "Exactitude: 0.4780701754385965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Mapping des émotions numériques vers leurs noms\n",
        "mapping_emotions = {\n",
        "    1: 'fear',\n",
        "    2: 'anger',\n",
        "    3: 'joy',\n",
        "    4: 'surprise',\n",
        "    5: 'sadness',\n",
        "    6: 'disgust',\n",
        "    7: 'neutral'\n",
        "}\n",
        "\n",
        "# Mapping inverse pour la comparaison\n",
        "mapping_inverse = {\n",
        "    'fear': 1,\n",
        "    'anger': 2,\n",
        "    'joy': 3,\n",
        "    'surprise': 4,\n",
        "    'sadness': 5,\n",
        "    'disgust': 6,\n",
        "    'neutral': 7\n",
        "}\n",
        "\n",
        "# Chargement du modèle et du tokenizer\n",
        "print(\"Chargement du modèle et du tokenizer EmoBERTa...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tae898/emoberta-large\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"tae898/emoberta-large\")\n",
        "emotion_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "csv_path = '/content/drive/MyDrive/Annotation_final.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Initialisation du stemmer anglais\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Fonction de nettoyage et stemming\n",
        "def nettoyer_texte_stemmer(texte):\n",
        "    texte = str(texte).lower()\n",
        "    texte = re.sub(r\"http\\S+|www\\S+\", \"\", texte)\n",
        "    texte = re.sub(r\"@\\w+|#\\w+\", \"\", texte)\n",
        "    texte = re.sub(r\"\\s+\", \" \", texte).strip()\n",
        "    mots = texte.split()\n",
        "    mots_stemmes = [stemmer.stem(mot) for mot in mots]\n",
        "    return \" \".join(mots_stemmes)\n",
        "\n",
        "# Application du prétraitement\n",
        "print(\"Application du prétraitement...\")\n",
        "df['texte_nettoye'] = df['text'].apply(nettoyer_texte_stemmer)\n",
        "\n",
        "# Limitation à 300 entrées pour les tests\n",
        "#df = df.head(500)\n",
        "\n",
        "# Prédiction des émotions avec EmoBERTa\n",
        "print(\"Prédiction des émotions...\")\n",
        "resultats = []\n",
        "\n",
        "for texte in df['texte_nettoye']:\n",
        "    try:\n",
        "        prediction = emotion_pipeline(texte)[0]\n",
        "        emotion_label = prediction['label']\n",
        "        emotion_num = mapping_inverse.get(emotion_label, 0)\n",
        "        resultats.append({\n",
        "            'texte': texte,\n",
        "            'emotion_originale': emotion_label,\n",
        "            'emotion_num': emotion_num,\n",
        "            'emotion': mapping_emotions.get(emotion_num, 'inconnu'),\n",
        "            'score': prediction['score']\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la prédiction pour le texte: {texte}\")\n",
        "        print(f\"Erreur: {e}\")\n",
        "        resultats.append({\n",
        "            'texte': texte,\n",
        "            'emotion_originale': 'erreur',\n",
        "            'emotion_num': 0,\n",
        "            'emotion': 'erreur',\n",
        "            'score': 0.0\n",
        "        })\n",
        "\n",
        "# Création d'un DataFrame avec les résultats\n",
        "resultats_df = pd.DataFrame(resultats)\n",
        "\n",
        "# Fusion avec le DataFrame original\n",
        "df_final = pd.concat([df, resultats_df[['emotion_originale', 'emotion_num', 'emotion', 'score']]], axis=1)\n",
        "\n",
        "# Évaluation des performances avec la colonne 'emotions' comme référence\n",
        "print(\"Évaluation des performances avec la colonne 'emotions'...\")\n",
        "y_true = df['emotions']\n",
        "y_pred = resultats_df['emotion_num']\n",
        "\n",
        "print(\"Rapport de classification:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[mapping_emotions[i] for i in sorted(mapping_emotions.keys())]))\n",
        "\n",
        "print(f\"Précision: {precision_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"Rappel: {recall_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"F1-score: {f1_score(y_true, y_pred, average='weighted', zero_division=0)}\")\n",
        "print(f\"Exactitude: {accuracy_score(y_true, y_pred)}\")\n",
        "\n",
        "# Sauvegarde des résultats\n",
        "df_final.to_csv('/content/drive/MyDrive/resultats_emoberta.csv', index=False)\n",
        "print(\"Résultats sauvegardés dans 'resultats_emoberta.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMiNCbaWO7hs",
        "outputId": "36297b3e-f4e9-4bce-f497-76a479c708ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement du modèle et du tokenizer EmoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Application du prétraitement...\n",
            "Prédiction des émotions...\n",
            "Évaluation des performances avec la colonne 'emotions'...\n",
            "Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fear       0.00      0.00      0.00        52\n",
            "       anger       0.62      0.67      0.64       184\n",
            "         joy       0.56      0.53      0.54       218\n",
            "    surprise       0.83      0.05      0.10        91\n",
            "     sadness       0.44      0.31      0.36        65\n",
            "     disgust       0.43      0.07      0.11        46\n",
            "     neutral       0.57      0.80      0.67       484\n",
            "\n",
            "    accuracy                           0.57      1140\n",
            "   macro avg       0.49      0.35      0.35      1140\n",
            "weighted avg       0.56      0.57      0.52      1140\n",
            "\n",
            "Précision: 0.5580855379438375\n",
            "Rappel: 0.5719298245614035\n",
            "F1-score: 0.5235125579980914\n",
            "Exactitude: 0.5719298245614035\n",
            "Résultats sauvegardés dans 'resultats_emoberta.csv'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}